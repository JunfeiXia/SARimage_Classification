{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "print(\"Python Version:\", tf.__version__)\n",
    "# print(\"Keras Version:\", tf.keras.__version__)\n",
    "print(\"CuDNN Version:\", tf.sysconfig.get_build_info())\n",
    "print(\"CUDA Version:\", tf.sysconfig.get_build_info())\n",
    "\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "def seed_everything(seed = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "gpu_id = '0'  # or '0,1' for multiple GPUs\n",
    "\n",
    "# Set GPU device\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.set_visible_devices(physical_devices[int(gpu_id)], 'GPU')\n",
    "    # Optionally, you can set memory growth to avoid allocating all GPU memory at once\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[int(gpu_id)], True)\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "batch_size = 8\n",
    "n_classes = 10\n",
    "EPOCHS = 50\n",
    "\n",
    "train_path = '/mnt/e/Projects/SAR_Training/Ten_events_classification/Data/Balanced/Train'\n",
    "# val_path = '/mnt/e/Projects/SAR_Training/Ten_events_classification/Data/Balanced/Val' # combined in training\n",
    "\n",
    "\n",
    "classes = {0 : \"Pure Ocean Waves\",\n",
    "           1 : \"Wind Streaks\",\n",
    "           2 : \"Micro Convective Cells\",\n",
    "           3 : \"Rain Cells\",\n",
    "           4 : \"Biological Slicks\",\n",
    "           5 : \"Sea Ice\",\n",
    "           6 : \"Icebergs\",\n",
    "           7 : \"Low Wind Area\",\n",
    "           8 : \"Atmospheric Front\",\n",
    "           9 : \"Oceanic Front\",\n",
    "           }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    seed_everything()\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    \n",
    "    # Flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    # Pixel-level transforms\n",
    "    if p_pixel_1 >= .4:\n",
    "        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n",
    "    if p_pixel_2 >= .4:\n",
    "        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n",
    "    if p_pixel_3 >= .4:\n",
    "        image = tf.image.random_brightness(image, max_delta = .1)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling( future... now just manually removed some data and make them balanced,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          validation_split = 0.3,\n",
    "                                                          preprocessing_function = data_augment,\n",
    "                                                          )\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     rescale=1./255,  # Rescale pixel values for neural network compatibility\n",
    "#     samplewise_center=True,  # Center the data sample-wise (mean = 0)\n",
    "#     samplewise_std_normalization=True,  # Normalize the data sample-wise (std = 1)\n",
    "#     validation_split=0.3,  # Reserve 20% of the data for validation\n",
    "#     # preprocessing_function=data_augment,  # Custom preprocessing/augmentation function\n",
    "#     rotation_range=180,  # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "#     width_shift_range=0.2,  # Randomly shift images horizontally (fraction of total width)\n",
    "#     height_shift_range=0.2,  # Randomly shift images vertically (fraction of total height)\n",
    "#     shear_range=0.1,  # Apply shearing transformations\n",
    "#     zoom_range=0.2,  # Randomly zooming inside pictures\n",
    "#     horizontal_flip=True,  # Randomly flip images horizontally\n",
    "#     fill_mode='nearest'  # Strategy used for filling in newly created pixels\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# set as training data\n",
    "train_gen  = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(384, 384),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True,\n",
    "    class_mode='categorical',\n",
    "    subset='training') \n",
    "\n",
    "# same directory as validation data\n",
    "\n",
    "valid_gen  = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(384, 384),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = False,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "images = [train_gen[0][0][i] for i in range(8)]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize = (10, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images, axes):\n",
    "    ax.imshow(img.reshape(image_size, image_size, 3).astype(\"uint8\"))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit,utils, visualize\n",
    "\n",
    "# vit_model = vit.vit_b16(\n",
    "#         image_size = image_size,\n",
    "#         activation = 'softmax',\n",
    "#         pretrained = True,\n",
    "#         # include_top = False,\n",
    "#         include_top = True,\n",
    "#         pretrained_top = False,\n",
    "#         classes = 10)\n",
    "\n",
    "vit_model = vit.vit_l32(\n",
    "    image_size=image_size,\n",
    "    activation='sigmoid',\n",
    "    pretrained=True,\n",
    "    include_top=True,\n",
    "    pretrained_top=False,\n",
    "    classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # warnings.filterwarnings(\"ignore\")\n",
    "# from vit_keras import vit,utils, visualize\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# # Load a model\n",
    "# image_size = 384\n",
    "# classes = utils.get_imagenet_classes()\n",
    "# model = vit.vit_b16(\n",
    "#     image_size=image_size,\n",
    "#     activation='sigmoid',\n",
    "#     pretrained=True,\n",
    "#     include_top=True,\n",
    "#     pretrained_top=True\n",
    "# )\n",
    "# classes = utils.get_imagenet_classes()\n",
    "\n",
    "# # Get an image and compute the attention map\n",
    "# # url = 'https://upload.wikimedia.org/wikipedia/commons/b/bc/Free%21_%283987584939%29.jpg'\n",
    "# # image = utils.read(url, image_size)\n",
    "# path = './PNG/O/s1a-wv1-slc-vv-20160102t223009-20160102t223012-009321-00d78c-017.png'\n",
    "# image = utils.read(path, image_size)\n",
    "# attention_map = visualize.attention_map(model=model, image=image)\n",
    "# print('Prediction:', classes[\n",
    "#     model.predict(vit.preprocess_inputs(image)[np.newaxis])[0].argmax()]\n",
    "# )  # Prediction: Eskimo dog, husky\n",
    "\n",
    "# # Plot results\n",
    "# fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "# ax1.axis('off')\n",
    "# ax2.axis('off')\n",
    "# ax1.set_title('Original')\n",
    "# ax2.set_title('Attention Map')\n",
    "# _ = ax1.imshow(image)\n",
    "# _ = ax2.imshow(attention_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "batch_size = 8\n",
    "patch_size = 12  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "x = train_gen.next()\n",
    "image = x[0][0]\n",
    "\n",
    "plt.imshow(image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size = (image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f'Image size: {image_size} X {image_size}')\n",
    "print(f'Patch size: {patch_size} X {patch_size}')\n",
    "print(f'Patches per image: {patches.shape[1]}')\n",
    "print(f'Elements per patch: {patches.shape[-1]}')\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype('uint8'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_frequencies = [4900, 4797, 4598, 4740, 4709, 4370, 1980, 2160, 4100, 1199]\n",
    "# total_data_points = np.sum(class_frequencies)\n",
    "# initial_biases = [np.log(total_data_points / freq) for freq in class_frequencies]\n",
    "# initial_bias = tf.constant(initial_biases, dtype=tf.float32)\n",
    "# custom_bias_initializer = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(64, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.Dense(32, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.Dense(10, 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy, Policy\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# class_weight = { \n",
    "#     0: 4900/4900,\n",
    "#     1: 4900/4797,\n",
    "#     2: 4900/4598,\n",
    "#     3: 4900/4740,\n",
    "#     4: 4900/4709,\n",
    "#     5: 4900/4370,\n",
    "#     6: 4900/1980,\n",
    "#     7: 4900/2160,\n",
    "#     8: 4900/4100,\n",
    "#     9: 4900/1199,\n",
    "# }\n",
    "\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
    "\n",
    "checkpoint_path = \"./checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_dir,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          validation_data = valid_gen,\n",
    "          validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks = early_stopping_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# os.listdir(checkpoint_dir)\n",
    "\n",
    "import pickle\n",
    "class History_trained_model(object):\n",
    "    def __init__(self, history, epoch, params):\n",
    "        self.history = history\n",
    "        self.epoch = epoch\n",
    "        self.params = params\n",
    "\n",
    "with open('/mnt/e/Projects/SAR_Training/Ten_events_classification/'+'/ViTmodel_l32_384_T7V3_320_history', 'wb') as file:\n",
    "    model_history= History_trained_model(history.history, history.epoch, history.params)\n",
    "    pickle.dump(model_history, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open(savemodel_path+'/history', 'rb') as file:\n",
    "#     history=pickle.load(file)\n",
    "\n",
    "# print(history.history)\n",
    "\n",
    "model.save('/mnt/e/Projects/SAR_Training/Ten_events_classification/ViTmodel_l32_384_T7V3_320.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViT Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/mnt/e/Projects/SAR_Training/Ten_events_classification/Data/Balanced/Test'\n",
    "\n",
    "testdatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255  # Rescale pixel values for neural network compatibility\n",
    "    # samplewise_center=True,  # Center the data sample-wise (mean = 0)\n",
    "    # samplewise_std_normalization=True,  # Normalize the data sample-wise (std = 1)\n",
    "    # validation_split=0.2,  # Reserve 20% of the data for validation\n",
    "    # # preprocessing_function=data_augment,  # Custom preprocessing/augmentation function\n",
    "    # rotation_range=180,  # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # width_shift_range=0.2,  # Randomly shift images horizontally (fraction of total width)\n",
    "    # height_shift_range=0.2,  # Randomly shift images vertically (fraction of total height)\n",
    "    # shear_range=0.2,  # Apply shearing transformations\n",
    "    # zoom_range=0.2,  # Randomly zooming inside pictures\n",
    "    # horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    # fill_mode='nearest'  # Strategy used for filling in newly created pixels\n",
    ")\n",
    "\n",
    "test_gen  = testdatagen.flow_from_directory(\n",
    "    test_path ,\n",
    "    target_size=(384, 384),\n",
    "    batch_size = batch_size,\n",
    "    seed = 1,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = False,\n",
    "    # shuffle = True,\n",
    "    # class_mode='categorical',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(model.predict(test_gen, steps = test_gen.n // test_gen.batch_size + 1), axis = 1)\n",
    "true_classes = test_gen.classes\n",
    "class_labels = list(test_gen.class_indices.keys())  \n",
    "\n",
    "confusionmatrix = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize = (16, 16))\n",
    "sns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n",
    "\n",
    "print(classification_report(true_classes, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
